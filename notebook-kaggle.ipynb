{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":2157,"sourceType":"datasetVersion","datasetId":18}],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-06-20T15:07:06.417780Z","iopub.execute_input":"2024-06-20T15:07:06.418156Z","iopub.status.idle":"2024-06-20T15:07:06.431888Z","shell.execute_reply.started":"2024-06-20T15:07:06.418129Z","shell.execute_reply":"2024-06-20T15:07:06.430857Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/hashes.txt\n/kaggle/input/Reviews.csv\n/kaggle/input/database.sqlite\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install transformers datasets scikit-learn torch","metadata":{"execution":{"iopub.status.busy":"2024-06-19T15:53:09.855237Z","iopub.execute_input":"2024-06-19T15:53:09.855637Z","iopub.status.idle":"2024-06-19T15:53:22.959864Z","shell.execute_reply.started":"2024-06-19T15:53:09.855612Z","shell.execute_reply":"2024-06-19T15:53:22.958734Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.41.2)\nRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.19.2)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (1.2.2)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.1.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.13.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.23.2)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.12.25)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.19.1)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.3)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.4)\nRequirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (14.0.2)\nRequirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets) (0.6)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.2.1)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.3.1,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.3.1,>=2023.1.0->datasets) (2024.3.1)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.1)\nRequirement already satisfied: scipy>=1.3.2 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.11.4)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (3.2.0)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.12.1)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.2)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.2.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.3)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.4)\nRequirement already satisfied: mpmath<1.4.0,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nfrom transformers import BertModel, BertPreTrainedModel\nfrom torch import nn\n\nclass BERTCNN(BertPreTrainedModel):\n    def __init__(self, config):\n        super(BERTCNN, self).__init__(config)\n        self.bert = BertModel.from_pretrained(\"bert-base-uncased\", config=config)\n        self.conv = nn.Conv1d(in_channels=config.hidden_size, out_channels=128, kernel_size=5, padding=2)\n        self.pool = nn.AdaptiveMaxPool1d(1)\n        self.dropout = nn.Dropout(0.5)\n        self.classifier = nn.Linear(128, config.num_labels)\n        self.init_weights()\n\n    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None, inputs_embeds=None, labels=None):\n        outputs = self.bert(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, position_ids=position_ids, head_mask=head_mask, inputs_embeds=inputs_embeds)\n        sequence_output = outputs[0]\n        sequence_output = sequence_output.permute(0, 2, 1)\n        x = self.conv(sequence_output)\n        x = self.pool(x).squeeze(-1)\n        x = self.dropout(x)\n        logits = self.classifier(x)\n\n        loss = None\n        if labels is not None:\n            loss_fct = nn.CrossEntropyLoss()\n            loss = loss_fct(logits.view(-1, self.config.num_labels), labels.view(-1))\n\n        return (loss, logits) if loss is not None else logits","metadata":{"execution":{"iopub.status.busy":"2024-06-19T15:53:22.961324Z","iopub.execute_input":"2024-06-19T15:53:22.961658Z","iopub.status.idle":"2024-06-19T15:53:28.021567Z","shell.execute_reply.started":"2024-06-19T15:53:22.961632Z","shell.execute_reply":"2024-06-19T15:53:28.020797Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom torch import nn\nfrom transformers import BertModel, BertPreTrainedModel, BertConfig, AutoTokenizer, TrainingArguments, Trainer\n\nclass TransBLSTM(BertPreTrainedModel):\n    def __init__(self, config):\n        super(TransBLSTM, self).__init__(config)\n        self.bert = BertModel.from_pretrained(\"bert-base-uncased\", config=config)\n        self.blstm = nn.LSTM(config.hidden_size, config.hidden_size // 2, \n                             num_layers=1, bidirectional=True, batch_first=True)\n        self.layer_norm = nn.LayerNorm(config.hidden_size)\n        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n        self.dropout = nn.Dropout(0.5)\n        self.init_weights()\n\n    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None, \n                position_ids=None, head_mask=None, inputs_embeds=None, labels=None):\n        bert_outputs = self.bert(input_ids, attention_mask=attention_mask, \n                                 token_type_ids=token_type_ids, position_ids=position_ids, \n                                 head_mask=head_mask, inputs_embeds=inputs_embeds)\n        \n        sequence_output = bert_outputs[0]\n        blstm_output, _ = self.blstm(sequence_output)\n        combined_output = self.layer_norm(sequence_output + blstm_output)\n        \n        pooled_output = combined_output[:, 0]\n        pooled_output = self.dropout(pooled_output)\n        logits = self.classifier(pooled_output)\n\n        loss = None\n        if labels is not None:\n            loss_fct = nn.CrossEntropyLoss()\n            loss = loss_fct(logits.view(-1, self.config.num_labels), labels.view(-1))\n        return (loss, logits) if loss is not None else logits\n","metadata":{"execution":{"iopub.status.busy":"2024-06-19T15:53:28.023981Z","iopub.execute_input":"2024-06-19T15:53:28.024385Z","iopub.status.idle":"2024-06-19T15:53:39.459645Z","shell.execute_reply.started":"2024-06-19T15:53:28.024361Z","shell.execute_reply":"2024-06-19T15:53:39.458887Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"2024-06-19 15:53:30.370947: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-06-19 15:53:30.371072: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-06-19 15:53:30.489949: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import AutoModelForSequenceClassification\n\nclass RoBERTa:\n    def __init__(self, model_type='cardiffnlp/twitter-roberta-base-sentiment', num_labels=3):\n        self.model = AutoModelForSequenceClassification.from_pretrained(model_type, num_labels=num_labels)\n\n    def get_model(self):\n        return self.model","metadata":{"execution":{"iopub.status.busy":"2024-06-19T15:53:39.460735Z","iopub.execute_input":"2024-06-19T15:53:39.461277Z","iopub.status.idle":"2024-06-19T15:53:39.466576Z","shell.execute_reply.started":"2024-06-19T15:53:39.461250Z","shell.execute_reply":"2024-06-19T15:53:39.465565Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"!pip install transformers[torch] accelerate -U","metadata":{"execution":{"iopub.status.busy":"2024-06-19T15:53:39.467842Z","iopub.execute_input":"2024-06-19T15:53:39.468181Z","iopub.status.idle":"2024-06-19T15:53:52.303884Z","shell.execute_reply.started":"2024-06-19T15:53:39.468152Z","shell.execute_reply":"2024-06-19T15:53:52.302924Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.30.1)\nCollecting accelerate\n  Downloading accelerate-0.31.0-py3-none-any.whl.metadata (19 kB)\nRequirement already satisfied: transformers[torch] in /opt/conda/lib/python3.10/site-packages (4.41.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (3.13.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (0.23.2)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (2023.12.25)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (2.32.3)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (0.19.1)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (0.4.3)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (4.66.4)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (2.1.2)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.3)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.0->transformers[torch]) (2024.3.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.0->transformers[torch]) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers[torch]) (3.1.1)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->transformers[torch]) (1.12.1)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->transformers[torch]) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->transformers[torch]) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers[torch]) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers[torch]) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers[torch]) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers[torch]) (2024.2.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->transformers[torch]) (2.1.3)\nRequirement already satisfied: mpmath<1.4.0,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->transformers[torch]) (1.3.0)\nDownloading accelerate-0.31.0-py3-none-any.whl (309 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m309.4/309.4 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: accelerate\n  Attempting uninstall: accelerate\n    Found existing installation: accelerate 0.30.1\n    Uninstalling accelerate-0.30.1:\n      Successfully uninstalled accelerate-0.30.1\nSuccessfully installed accelerate-0.31.0\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nfrom transformers import AutoTokenizer, TrainingArguments, Trainer, DataCollatorWithPadding, BertConfig\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score\nfrom datasets import Dataset\nimport torch\n\ndef load_data(file_path):\n    df = pd.read_csv(file_path, on_bad_lines='skip', nrows=70000)\n    df['Sentiment'] = df['Score'].apply(map_score_to_sentiment)\n    return train_test_split(df[['Text', 'Sentiment']], test_size=0.4, random_state=42)\n\ndef map_score_to_sentiment(score):\n    return 0 if score < 3 else (1 if score == 3 else 2)\n\ndef tokenize_data(tokenizer, texts, labels):\n    tokenized_inputs = tokenizer(texts.tolist(), padding=\"max_length\", truncation=True, max_length=512)\n    return Dataset.from_dict({**tokenized_inputs, 'labels': labels.tolist()})\n\n\ndef compute_metrics(p):\n    preds = np.argmax(p.predictions, axis=1)\n    return {\"f1\": f1_score(p.label_ids, preds, average='macro')}\n\ndef train_model(model, train_dataset, test_dataset, tokenizer, output_dir):\n    training_args = TrainingArguments(\n        output_dir=output_dir,\n        report_to=\"none\",\n        num_train_epochs=3,\n        per_device_train_batch_size=16,\n        per_device_eval_batch_size=64,\n        warmup_steps=500,\n        weight_decay=0.01,\n        logging_dir='./logs',\n        logging_steps=10,\n        save_total_limit=2,\n        save_steps=500,\n        eval_strategy=\"steps\",\n        eval_steps=500,\n        load_best_model_at_end=True,\n        metric_for_best_model=\"f1\" \n    )\n\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_dataset,\n        eval_dataset=test_dataset,\n        compute_metrics=compute_metrics\n    )\n\n    trainer.train()\n\n    model_path = f\"{output_dir}/best_model\"\n    model.save_pretrained(model_path)\n    tokenizer.save_pretrained(model_path)","metadata":{"execution":{"iopub.status.busy":"2024-06-19T15:53:52.305424Z","iopub.execute_input":"2024-06-19T15:53:52.305720Z","iopub.status.idle":"2024-06-19T15:53:52.331152Z","shell.execute_reply.started":"2024-06-19T15:53:52.305694Z","shell.execute_reply":"2024-06-19T15:53:52.330198Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"train_df, test_df = load_data('/kaggle/input/Reviews.csv')\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\ntrain_dataset = tokenize_data(tokenizer, train_df['Text'], train_df['Sentiment'])\ntest_dataset = tokenize_data(tokenizer, test_df['Text'], test_df['Sentiment'])\n\ndef data_stats(data_df):\n    num_reviews = data_df.shape[0]\n    print(f\"Total number of reviews: {num_reviews}\")\n\n    lengths = data_df['Text'].apply(len)\n    average_length = lengths.mean()\n    print(f\"Average review length: {average_length:.2f} characters\")\n\n    all_words = ' '.join(data_df['Text']).split()\n    vocab_size = len(set(all_words))\n    print(f\"Vocabulary size: {vocab_size}\")\n\n    median_length = lengths.median()\n    min_length = lengths.min()\n    max_length = lengths.max()\n    print(f\"Median review length: {median_length} characters\")\n    print(f\"Minimum review length: {min_length} characters\")\n    print(f\"Maximum review length: {max_length} characters\")\n\n\n    negative_count = len([negative for negative in data_df['Sentiment'] if negative == 0])\n    neutral_count = len([neutral for neutral in data_df['Sentiment'] if neutral == 1])\n    positive_count = len([positive for positive in data_df['Sentiment'] if positive == 2])\n    print(f'Negative Count: {negative_count}')\n    print(f'Neutral Count: {neutral_count}')\n    print(f'Positive Count: {positive_count}')\n\nprint(\"Training Data Statistics:\")\ndata_stats(train_df)\nprint(\"\\nTesting Data Statistics:\")\ndata_stats(test_df)","metadata":{"execution":{"iopub.status.busy":"2024-06-19T15:53:52.332277Z","iopub.execute_input":"2024-06-19T15:53:52.332532Z","iopub.status.idle":"2024-06-19T15:54:36.271117Z","shell.execute_reply.started":"2024-06-19T15:53:52.332491Z","shell.execute_reply":"2024-06-19T15:54:36.270188Z"},"trusted":true},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9425720c64f34dfbafdb92c87aa9b388"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9482f549cfe7438cae1cb48fe94de270"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9c16dfd472894f13a882bb642add6cf4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bbe4cf8471114ffd86b41a9af4f44553"}},"metadata":{}},{"name":"stdout","text":"Training Data Statistics:\nTotal number of reviews: 42000\nAverage review length: 435.00 characters\nVocabulary size: 131319\nMedian review length: 305.0 characters\nMinimum review length: 33 characters\nMaximum review length: 10327 characters\nNegative Count: 6306\nNeutral Count: 3317\nPositive Count: 32377\n\nTesting Data Statistics:\nTotal number of reviews: 28000\nAverage review length: 436.48 characters\nVocabulary size: 102562\nMedian review length: 307.0 characters\nMinimum review length: 44 characters\nMaximum review length: 16952 characters\nNegative Count: 4231\nNeutral Count: 2221\nPositive Count: 21548\n","output_type":"stream"}]},{"cell_type":"code","source":"# bert_model_type = 'bert-base-uncased'\n# bert_cnn_config = BertConfig.from_pretrained(bert_model_type, num_labels=3)\n# bert_cnn_model = BERTCNN(config=bert_cnn_config)\n# train_model(bert_cnn_model, train_dataset, test_dataset, tokenizer, './bert_cnn_results')","metadata":{"execution":{"iopub.status.busy":"2024-06-19T15:54:36.272200Z","iopub.execute_input":"2024-06-19T15:54:36.272488Z","iopub.status.idle":"2024-06-19T15:54:36.276636Z","shell.execute_reply.started":"2024-06-19T15:54:36.272463Z","shell.execute_reply":"2024-06-19T15:54:36.275772Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"bert_model_type = \"bert-base-uncased\"\ntrans_blstm_config = BertConfig.from_pretrained(bert_model_type, num_labels=3)\ntrans_blstm_model = TransBLSTM.from_pretrained(bert_model_type, config=trans_blstm_config)\ntrain_model(trans_blstm_model, train_dataset, test_dataset, tokenizer, \"./trans_blstm_model\")","metadata":{"execution":{"iopub.status.busy":"2024-06-19T15:54:36.279623Z","iopub.execute_input":"2024-06-19T15:54:36.279907Z","iopub.status.idle":"2024-06-19T20:14:26.892777Z","shell.execute_reply.started":"2024-06-19T15:54:36.279885Z","shell.execute_reply":"2024-06-19T20:14:26.891907Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6b5445ba6d654b36a7a53fcdea8b81d9"}},"metadata":{}},{"name":"stderr","text":"Some weights of TransBLSTM were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['blstm.bias_hh_l0', 'blstm.bias_hh_l0_reverse', 'blstm.bias_ih_l0', 'blstm.bias_ih_l0_reverse', 'blstm.weight_hh_l0', 'blstm.weight_hh_l0_reverse', 'blstm.weight_ih_l0', 'blstm.weight_ih_l0_reverse', 'classifier.bias', 'classifier.weight', 'layer_norm.bias', 'layer_norm.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='7875' max='7875' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [7875/7875 4:19:45, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>0.415100</td>\n      <td>0.387679</td>\n      <td>0.560522</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.279900</td>\n      <td>0.319350</td>\n      <td>0.691528</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>0.269200</td>\n      <td>0.300247</td>\n      <td>0.696126</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>0.235700</td>\n      <td>0.304391</td>\n      <td>0.725081</td>\n    </tr>\n    <tr>\n      <td>2500</td>\n      <td>0.274200</td>\n      <td>0.338783</td>\n      <td>0.650696</td>\n    </tr>\n    <tr>\n      <td>3000</td>\n      <td>0.259200</td>\n      <td>0.311768</td>\n      <td>0.737709</td>\n    </tr>\n    <tr>\n      <td>3500</td>\n      <td>0.234000</td>\n      <td>0.330907</td>\n      <td>0.702581</td>\n    </tr>\n    <tr>\n      <td>4000</td>\n      <td>0.192700</td>\n      <td>0.316016</td>\n      <td>0.736578</td>\n    </tr>\n    <tr>\n      <td>4500</td>\n      <td>0.079400</td>\n      <td>0.332638</td>\n      <td>0.746636</td>\n    </tr>\n    <tr>\n      <td>5000</td>\n      <td>0.226000</td>\n      <td>0.278982</td>\n      <td>0.754214</td>\n    </tr>\n    <tr>\n      <td>5500</td>\n      <td>0.081200</td>\n      <td>0.388148</td>\n      <td>0.757836</td>\n    </tr>\n    <tr>\n      <td>6000</td>\n      <td>0.050300</td>\n      <td>0.451801</td>\n      <td>0.753532</td>\n    </tr>\n    <tr>\n      <td>6500</td>\n      <td>0.192200</td>\n      <td>0.446824</td>\n      <td>0.752840</td>\n    </tr>\n    <tr>\n      <td>7000</td>\n      <td>0.174200</td>\n      <td>0.412383</td>\n      <td>0.761211</td>\n    </tr>\n    <tr>\n      <td>7500</td>\n      <td>0.241600</td>\n      <td>0.435529</td>\n      <td>0.764801</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}}]},{"cell_type":"code","source":"# roberta_model_type = 'cardiffnlp/twitter-roberta-base-sentiment'\n# roberta_tokenizer = AutoTokenizer.from_pretrained(roberta_model_type)\n# roberta_train_dataset = tokenize_data(roberta_tokenizer,  train_df['Text'], train_df['Sentiment'])\n# roberta_test_dataset = tokenize_data(roberta_tokenizer,  test_df['Text'], test_df['Sentiment'])\n\n# roberta_model = RoBERTa(model_type=roberta_model_type).get_model()\n# train_model(roberta_model, roberta_train_dataset, roberta_test_dataset, tokenizer, './roberta_results')","metadata":{"execution":{"iopub.status.busy":"2024-06-19T20:14:26.894130Z","iopub.execute_input":"2024-06-19T20:14:26.894393Z","iopub.status.idle":"2024-06-19T20:14:26.898936Z","shell.execute_reply.started":"2024-06-19T20:14:26.894371Z","shell.execute_reply":"2024-06-19T20:14:26.898025Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"import numpy as np","metadata":{"execution":{"iopub.status.busy":"2024-06-19T20:24:31.642170Z","iopub.execute_input":"2024-06-19T20:24:31.642494Z","iopub.status.idle":"2024-06-19T20:24:31.646961Z","shell.execute_reply.started":"2024-06-19T20:24:31.642471Z","shell.execute_reply":"2024-06-19T20:24:31.645990Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"def test_model(model, test_dataset):\n    trainer = Trainer(model=model)\n    result = trainer.predict(test_dataset)\n    prediction = np.argmax(result.predictions, axis=1)\n    return result, prediction","metadata":{"execution":{"iopub.status.busy":"2024-06-19T20:24:36.475301Z","iopub.execute_input":"2024-06-19T20:24:36.475695Z","iopub.status.idle":"2024-06-19T20:24:36.480951Z","shell.execute_reply.started":"2024-06-19T20:24:36.475664Z","shell.execute_reply":"2024-06-19T20:24:36.479986Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"# bert_cnn_result, bert_cnn_preds = test_model(bert_cnn_model, test_dataset)","metadata":{"execution":{"iopub.status.busy":"2024-06-19T20:14:26.937407Z","iopub.execute_input":"2024-06-19T20:14:26.937794Z","iopub.status.idle":"2024-06-19T20:14:26.941437Z","shell.execute_reply.started":"2024-06-19T20:14:26.937762Z","shell.execute_reply":"2024-06-19T20:14:26.940537Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"trans_blstm_result, trans_blstm_preds = test_model(trans_blstm_model, test_dataset)","metadata":{"execution":{"iopub.status.busy":"2024-06-19T20:24:41.817246Z","iopub.execute_input":"2024-06-19T20:24:41.817617Z","iopub.status.idle":"2024-06-19T20:33:56.595833Z","shell.execute_reply.started":"2024-06-19T20:24:41.817588Z","shell.execute_reply":"2024-06-19T20:33:56.594861Z"},"trusted":true},"execution_count":26,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}}]},{"cell_type":"code","source":"# roberta_result, roberta_preds = test_model(roberta_model, roberta_test_dataset)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def compare(model_result, model_preds, model_type):\n    print(model_type)\n    cases = ['negative', 'neutral', 'positive']\n\n    predictions_map = {\n        'negative': [],\n        'neutral': [],\n        'positive': [],\n    }\n    truth_map = {\n        'negative': [],\n        'neutral': [],\n        'positive': [],\n    }\n    falsy_map = {\n        'negative': [],\n        'neutral': [],\n        'positive': [],\n    }\n\n    for i, (result, preds) in enumerate(zip(model_result, model_preds)):\n        score = test_df['Sentiment'].iloc[i]\n        truth_map[cases[score]].append(i)\n        predictions_map[cases[preds]].append(i)\n        if score > 0 and preds == 0:\n            falsy_map[cases[0]].append(i)\n        elif score != 1 and preds == 1:\n            falsy_map[cases[1]].append(i)\n        elif score <2 and preds == 2:\n            falsy_map[cases[2]].append(i)\n\n\n    total_data = len(predictions_map[cases[0]]) + len(predictions_map[cases[1]]) + len(predictions_map[cases[2]])\n\n    print(\"Predictions\")\n    print(f'Negative:{len(predictions_map[cases[0]])} | Neutral: {len(predictions_map[cases[1]])} | Positive: {len(predictions_map[cases[2]])}')\n    print(\"============\\n\")\n    print(\"Truth\")\n    print(f'Negative:{len(truth_map[cases[0]])} | Neutral: {len(truth_map[cases[1]])} | Positive: {len(truth_map[cases[2]])}')\n    print(\"============\\n\")\n    print(\"False Positives\")\n    print(f'Negative:{len(falsy_map[cases[0]])} ({len(falsy_map[cases[0]])/len(truth_map[cases[0]])*100})| Neutral: {len(falsy_map[cases[1]])} ({len(falsy_map[cases[1]])/len(truth_map[cases[1]])*100})| Positive: {len(falsy_map[cases[2]])} ({len(falsy_map[cases[2]])/len(truth_map[cases[2]])*100})')\n    \n    print(\"============\\n\\n\\n\")","metadata":{"execution":{"iopub.status.busy":"2024-06-19T20:34:24.084854Z","iopub.execute_input":"2024-06-19T20:34:24.085589Z","iopub.status.idle":"2024-06-19T20:34:24.096418Z","shell.execute_reply.started":"2024-06-19T20:34:24.085557Z","shell.execute_reply":"2024-06-19T20:34:24.095545Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"# compare(bert_cnn_result.predictions, bert_cnn_preds,'bert-cnn')\n# compare(roberta_result.predictions, roberta_preds, roberta_model_type)\ncompare(trans_blstm_result.predictions, trans_blstm_preds, 'trans-blstm')","metadata":{"execution":{"iopub.status.busy":"2024-06-19T20:34:30.551359Z","iopub.execute_input":"2024-06-19T20:34:30.552339Z","iopub.status.idle":"2024-06-19T20:34:31.059429Z","shell.execute_reply.started":"2024-06-19T20:34:30.552296Z","shell.execute_reply":"2024-06-19T20:34:31.058189Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stdout","text":"trans-blstm\nPredictions\nNegative:4125 | Neutral: 2257 | Positive: 21618\n============\n\nTruth\nNegative:4231 | Neutral: 2221 | Positive: 21548\n============\n\nFalse Positives\nNegative:701 (16.568187189789647)| Neutral: 1106 (49.797388563710044)| Positive: 881 (4.088546500835345)\n============\n\n\n\n","output_type":"stream"}]},{"cell_type":"code","source":"!zip -r roberta_results.zip /kaggle/working/trans_blstm_model/best_model/\n","metadata":{"execution":{"iopub.status.busy":"2024-06-20T15:12:31.928647Z","iopub.execute_input":"2024-06-20T15:12:31.929658Z","iopub.status.idle":"2024-06-20T15:12:59.384815Z","shell.execute_reply.started":"2024-06-20T15:12:31.929619Z","shell.execute_reply":"2024-06-20T15:12:59.383214Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"  adding: kaggle/working/trans_blstm_model/best_model/ (stored 0%)\n  adding: kaggle/working/trans_blstm_model/best_model/model.safetensors (deflated 7%)\n  adding: kaggle/working/trans_blstm_model/best_model/tokenizer.json (deflated 71%)\n  adding: kaggle/working/trans_blstm_model/best_model/config.json (deflated 50%)\n  adding: kaggle/working/trans_blstm_model/best_model/tokenizer_config.json (deflated 76%)\n  adding: kaggle/working/trans_blstm_model/best_model/special_tokens_map.json (deflated 42%)\n  adding: kaggle/working/trans_blstm_model/best_model/vocab.txt (deflated 53%)\n","output_type":"stream"}]},{"cell_type":"code","source":"from IPython.display import FileLink\nFileLink(r'roberta_results.zip')","metadata":{"execution":{"iopub.status.busy":"2024-06-20T15:13:28.447624Z","iopub.execute_input":"2024-06-20T15:13:28.448738Z","iopub.status.idle":"2024-06-20T15:13:28.456516Z","shell.execute_reply.started":"2024-06-20T15:13:28.448688Z","shell.execute_reply":"2024-06-20T15:13:28.455630Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"/kaggle/working/roberta_results.zip","text/html":"<a href='roberta_results.zip' target='_blank'>roberta_results.zip</a><br>"},"metadata":{}}]}]}